# Dealing with paginated responses

When you search on Google, did you notice that Google shows you ten articles per page? It doesn't show you 37 million articles at once.

Most servers act like Google. They send you an incomplete list because most users don't need so much information. There is no need to stress the server to provide more information than necessary.

But if you need more information, you can do things.

1. Increase the number of items per page
2. Fetch more pages

## Increasing the number of items per page

Many APIs let you decide how many items you can fetch per page. Github defaults to 30, but you can request up to 100 items per request. To do so, you use the `per_page` query parameter.

```js
fetch('https://api.github.com/users/zellwk/repos?per_page=100')
```

Different API use different query parameters. You must read the documentation to know which parameter to use, and the limit. Some APIs don't let you change the limit though, and that's also okay.

## Fetching more pages

If you know there are five pages, you can send five requests.

But let's say you want to fetch every resource. How do you work through the paginated responses?

That's what we're going to cover this lesson. It's going to be a bit long winded because this is my first time. I'm sure it'll become better as I go along.

There are two ways to do it:

1. Increase the number of responses per page
2. Perform multiple requests until you get all resources

I'm worried that Github won't allow unauthenticated responses once it uses Graph APIs. I want to work around this issue as much as possible, but I ack that this is not the time to think about it. I can look at this during my breaks or after work, or during another session. Not now, when I am trying to create another material. Put it aside for now. Gonna come back to it later.

## Increase the number of responses per page

Some APIs let you increase the number of responses you get with every request. Github, for example, lets you fetch up to 100 resources per page.

[Github page link] + Image

Different API gives you different number of responses. You need to read up on the API documentations to get a sense.
Some APIs, like Pokedex, use `limit` instead of `per_page`.

## Perform multiple requests

APIs that limit the number of responses often tell you that there's a next page. This information can usually be found in the headers or body.

Github gives you the information in the headers.

And some other example gives you information in the body?

Well' I need to work on some demo to be able to fetch this out, so I kinda have to work through a demo right now, or I won't be able to contitue.

But the general idea is:

1. Get the header
2. Check for Link
3. Get the page to parse for
4. Create a Promise.all
5. Wait for all promises to resolve
6. Do your thing

The things I need to test for are:

1. How to get headers in XHR and Fetch
2. How to get the Link header + body content in both methods
3. How to use the Link header to fetch resources

Then if there is no link. I need to work with recursion. Fetch until there are no more resources, or the number of resources don't match the limit you set.

This will mean explaining recursion. Actually I can write the thing first, then explain that this process is called recursion.

But well, have to test it out first too!

Gonna set per_page to 5. This will fetch more stuff.

Response headers => Chrome/Firefox devtools. Click on Network tab. Click on the resource you requested for. You will be able to see information, including headers, preview, response, and timing, query parameters (if any)

## Avoid fetching all resources.

What if there are 10,000 items? It's not worth the effort to parse through all 10,000 items if you only need one of them. You can create pagination too. Like Google. Maybe create one... Hmmm. Gotta think of it and check whether Dota supports paging. Otherwise, let's do a Pokedex paging.

We can also do a list-all for Pokedex, which can be fun. Then I'll talk about how you can do pagination with JavaScript ğŸ¤“

But anyway, you'll need the link header information.  Can organize later. Hm. Also need to think about where to teach reduce. This is becoming more of a challenge lol.

## Parsing the Link header

String. Need to figure out what we need. Here is where Regular Expressions and other String methods come in handy. We are not going to go into details about regular expressions, but you can find out more on regexone.com. They have an excellent tutorial about regex.

We first need to decide what we want to search for.

Regular Expressions is another way la. But we can use some string methods too.

Some useful string methods are:

1. split
2. replace

```js
const parseLinkHeader = (string) => {
  const arr = string.split(',')
  return arr.reduce((o, str) => {
    const parts = str.split(';')
    const link = parts[0].replace('<', '').replace('>', '').trim()
    const rel = parts[1].replace('rel="', '').replace('"', '').trim()
    return Object.assign({}, o, {
      [rel]: link
    })
  }, {})
}

```
